import streamlit as st
import pandas as pd
from github import Github
from openai import OpenAI
import hashlib
import re
import io
import time
import uuid
from datetime import datetime

# --- 1. æ ·å¼ä¸é…ç½® ---
st.set_page_config(page_title="æ€æ”¿åå¸ˆæ™ºèƒ½ç´ æåº“", layout="wide", page_icon="ğŸ›ï¸")

st.markdown("""
    <style>
    .stApp { background-color: #f8fafc; }
    mark { background-color: #ffff00 !important; color: #000 !important; padding: 0 3px; border-radius: 3px; font-weight: bold; }
    .important-red { color: #e11d48 !important; font-weight: bold; }
    .stExpander { border: 1px solid #e2e8f0 !important; border-radius: 12px !important; background: white !important; margin-bottom: 10px !important; }
    .book-tag { 
        background: #fee2e2; color: #b91c1c; padding: 2px 8px; border-radius: 4px; 
        font-size: 12px; font-weight: bold; display: block; margin-bottom: 5px; 
        text-align: center; border: 1px solid #fecaca;
    }
    </style>
    """, unsafe_allow_html=True)

# --- 2. æ ¸å¿ƒåç«¯å‡½æ•° ---
def get_github_repo():
    return Github(st.secrets["GH_TOKEN"]).get_repo(st.secrets["GH_REPO"])

def load_from_cloud(uid):
    file_path = f"material_lib_{uid}.csv"
    standard_cols = ["æ—¥æœŸ", "æ ‡é¢˜", "æ¶‰åŠæ•™æ", "è€ƒç‚¹è®¾é—®", "ç´ æåŸæ–‡"]
    try:
        repo = get_github_repo()
        content = repo.get_contents(file_path)
        # ã€åç¼“å­˜è¡¥ä¸ã€‘
        unique_req = str(uuid.uuid4())
        fresh_url = f"{content.download_url}?v={unique_req}"
        
        df = pd.read_csv(fresh_url)
        rename_map = {'ç´ ææ ‡é¢˜': 'æ ‡é¢˜', 'ç²¾ä¿®è§£æ': 'è€ƒç‚¹è®¾é—®', 'æ ¸å¿ƒè§£æ': 'è€ƒç‚¹è®¾é—®', 'æ¶‰åŠæ•™æ': 'æ¶‰åŠæ•™æ', 'åˆ†ç±»': 'æ¶‰åŠæ•™æ'}
        df.rename(columns=rename_map, inplace=True)
        for col in standard_cols:
            if col not in df.columns: df[col] = "æœªè®°å½•"
        return df[standard_cols], content.sha
    except:
        return pd.DataFrame(columns=standard_cols), None

# --- 3. ç™»å½•æ‹¦æˆªä¸çŠ¶æ€åˆå§‹åŒ– ---
if 'uid' not in st.session_state: st.session_state['uid'] = None
if 'display_df' not in st.session_state: st.session_state['display_df'] = None

if not st.session_state['uid']:
    st.markdown("<br><br>", unsafe_allow_html=True)
    col_l, col_m, col_r = st.columns([1, 2, 1])
    with col_m:
        st.title("ğŸ›ï¸ æ€æ”¿åå¸ˆå·¥ä½œå®¤")
        input_key = st.text_input("è¯·è¾“å…¥ API Key ç™»å½•", type="password")
        if st.button("ğŸš€ å¼€å¯å·¥ä½œå®¤", use_container_width=True):
            if len(input_key) > 10:
                st.session_state['api_key'] = input_key
                st.session_state['uid'] = hashlib.md5(input_key.encode()).hexdigest()[:8]
                st.rerun()
    st.stop()

uid = st.session_state['uid']
db_filename = f"material_lib_{uid}.csv"

# åˆå§‹åŒ–åŠ è½½å†…å­˜æ•°æ®
if st.session_state['display_df'] is None:
    df_init, current_sha = load_from_cloud(uid)
    st.session_state['display_df'] = df_init
    st.session_state['last_sha'] = current_sha

# --- 4. ä¾§è¾¹æ  ---
with st.sidebar:
    st.header(f"ğŸ‘¤ è€å¸ˆ ID: {uid}")
    if st.button("ğŸ”„ å¼ºåˆ¶é‡åˆ·äº‘ç«¯æ•°æ®", use_container_width=True):
        st.session_state['display_df'] = None
        st.rerun()
    st.divider()
    if not st.session_state['display_df'].empty:
        csv_io = io.BytesIO()
        st.session_state['display_df'].to_csv(csv_io, index=False, encoding='utf-8-sig')
        st.download_button("å¯¼å‡º CSV æ¸…å•", data=csv_io.getvalue(), file_name=f"æ€æ”¿æ™ºåº“_{datetime.now().strftime('%m%d')}.csv", use_container_width=True)
    if st.button("ğŸšª é€€å‡ºç™»å½•"):
        st.session_state.clear()
        st.rerun()

# --- 5. ä¸»åŠŸèƒ½åŒº ---
tab1, tab2 = st.tabs(["âœ¨ æ™ºèƒ½åŠ å·¥å½•å…¥", "ğŸ“‚ ç»“æ„åŒ–å…¨æ™¯çœ‹æ¿"])

with tab1:
    l_col, r_col = st.columns([1.2, 1])
    with l_col:
        with st.container(border=True):
            m_title = st.text_input("1. ç´ ææ ‡é¢˜")
            m_raw = st.text_area("2. ç´ æåŸæ–‡å†…å®¹", height=200)
            try:
                repo_obj = get_github_repo()
                pdf_list = [f.name.replace('.pdf', '').replace('.PDF', '') for f in repo_obj.get_contents("data") if f.name.lower().endswith('.pdf')]
            except:
                pdf_list = ["å¿…ä¿®1", "å¿…ä¿®2", "å¿…ä¿®3", "å¿…ä¿®4"]
            m_books = st.multiselect("3. å…³è”æ•™æ", options=sorted(pdf_list))
            
            if st.button("ğŸ§  å¼€å¯åå¸ˆæ•™ç ”åˆ†æ", use_container_width=True):
                if m_title and m_books and m_raw:
                    client = OpenAI(api_key=st.session_state['api_key'], base_url="https://api.deepseek.com")
                    with st.spinner("AIè”åŠ¨æ•™ç ”ä¸­..."):
                        prompt = f"ä½ æ˜¯æ€æ”¿åå¸ˆã€‚é’ˆå¯¹ã€Š{m_title}ã€‹ç»“åˆæ•™æ {m_books} åˆ†æã€‚ä¸¥ç¦åŠ ç²—ã€‚æ ¸å¿ƒè¯ç”¨<mark>ï¼Œç»“è®ºç”¨<span class='important-red'>ã€‚åŸæ–‡ï¼š{m_raw}"
                        resp = client.chat.completions.create(model="deepseek-chat", messages=[{"role":"user","content":prompt}])
                        st.session_state['ai_output'] = re.sub(r'\*\*(.*?)\*\*', r'<mark>\1</mark>', resp.choices[0].message.content)
                else: st.warning("è¯·å®Œæ•´å¡«å†™ç´ æå†…å®¹")

    with r_col:
        if 'ai_output' in st.session_state:
            st.markdown("âœï¸ **é¢„è§ˆä¸ç²¾ä¿®**")
            final_text = st.text_area("è§£æç»“æœ", value=st.session_state['ai_output'], height=450)
            if st.button("ğŸ’¾ ç¡®è®¤å½’æ¡£å…¥åº“", use_container_width=True):
                new_row = {"æ—¥æœŸ": datetime.now().strftime("%Y-%m-%d"), "æ ‡é¢˜": m_title, "æ¶‰åŠæ•™æ": " | ".join(m_books), "è€ƒç‚¹è®¾é—®": final_text, "ç´ æåŸæ–‡": m_raw}
                
                # --- å†…å­˜æŠ¢è·‘æ›´æ–° ---
                st.session_state['display_df'] = pd.concat([st.session_state['display_df'], pd.DataFrame([new_row])], ignore_index=True)
                
                # --- åŒæ­¥äº‘ç«¯ ---
                repo = get_github_repo()
                csv_str = st.session_state['display_df'].to_csv(index=False, encoding='utf-8-sig')
                _, latest_sha = load_from_cloud(uid)
                if latest_sha: repo.update_file(db_filename, "Update", csv_str, latest_sha)
                else: repo.create_file(db_filename, "Init", csv_str)
                
                st.success("âœ… å·²åŒæ­¥è‡³äº‘ç«¯çœ‹æ¿ï¼")
                if 'ai_output' in st.session_state: del st.session_state['ai_output']
                st.rerun()

with tab2:
    df_show = st.session_state['display_df']
    if not df_show.empty:
        # --- ã€è¡¨æ ¼åŠŸèƒ½å›å½’ã€‘ ---
        st.subheader("ğŸ“Š å¿«é€Ÿç´¢å¼•æ¸…å•")
        st.dataframe(df_show[["æ—¥æœŸ", "æ ‡é¢˜", "æ¶‰åŠæ•™æ"]], use_container_width=True, hide_index=True)
        st.divider()
        
        # --- è¯¦æƒ…æœç´¢ä¸çœ‹æ¿ ---
        st.subheader("ğŸ“– ç»“æ„åŒ–çœ‹æ¿è¯¦æƒ…")
        search = st.text_input("ğŸ” æœç´¢å…³é”®è¯ï¼ˆæ ‡é¢˜ã€æ•™æã€è€ƒç‚¹ï¼‰")
        filtered_df = df_show[df_show.apply(lambda r: r.astype(str).str.contains(search).any(), axis=1)] if search else df_show
        
        for i, row in filtered_df.iloc[::-1].iterrows():
            with st.expander(f"ğŸ“Œ {row['æ ‡é¢˜']} | {row['æ¶‰åŠæ•™æ']}"):
                c1, c2 = st.columns([1, 2.5])
                with c1:
                    st.markdown("**ğŸ“š æ¶‰åŠæ•™æ**")
                    for b in str(row['æ¶‰åŠæ•™æ']).split(" | "):
                        st.markdown(f"<span class='book-tag'>{b}</span>", unsafe_allow_html=True)
                with c2:
                    st.markdown("**ğŸ’¡ æ·±åº¦è”åŠ¨è§£æ**")
                    st.markdown(row['è€ƒç‚¹è®¾é—®'], unsafe_allow_html=True)
                st.divider()
                st.caption(f"ç´ æåŸæ–‡å‚è€ƒï¼š{row['ç´ æåŸæ–‡']}")
                if st.button(f"ğŸ—‘ï¸ åˆ é™¤æ­¤è®°å½•", key=f"del_{i}"):
                    st.session_state['display_df'] = st.session_state['display_df'].drop(i)
                    csv_str = st.session_state['display_df'].to_csv(index=False, encoding='utf-8-sig')
                    _, latest_sha = load_from_cloud(uid)
                    get_github_repo().update_file(db_filename, "Delete", csv_str, latest_sha)
                    st.rerun()
    else:
        st.info("åº“å†…æš‚æ— ç´ æã€‚")
